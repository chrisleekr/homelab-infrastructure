# https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml

## Create default rules for monitoring the cluster
##
defaultRules:
  create: true


## Install Prometheus Operator CRDs
##
crds:
  enabled: true
  ## The CRD upgrade job mitigates the limitation of helm not being able to upgrade CRDs.
  ## The job will apply the CRDs to the cluster before the operator is deployed, using helm hooks.
  ## It deploy a corresponding clusterrole, clusterrolebinding and serviceaccount to apply the CRDs.
  ## This feature is in preview, off by default and may change in the future.
  upgradeJob:
    enabled: true

## Configuration for alertmanager
## ref: https://prometheus.io/docs/alerting/alertmanager/
##
alertmanager:
  enabled: true
  ## Alertmanager configuration directives
  ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file
  ##      https://prometheus.io/webtools/alerting/routing-tree-editor/
  ##
  config:
    global:
      resolve_timeout: 5m
    inhibit_rules:
      - source_matchers:
          - 'severity = critical'
        target_matchers:
          - 'severity =~ warning|info'
        equal:
          - 'namespace'
          - 'alertname'
      - source_matchers:
          - 'severity = warning'
        target_matchers:
          - 'severity = info'
        equal:
          - 'namespace'
          - 'alertname'
      - source_matchers:
          - 'alertname = InfoInhibitor'
        target_matchers:
          - 'severity = info'
        equal:
          - 'namespace'
      - target_matchers:
          - 'alertname = InfoInhibitor'
    route:
      group_by: ['namespace']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: 'default-receiver'
      routes:
      - receiver: 'default-receiver'
        match:
          severity: critical
      - receiver: 'default-receiver'
        match:
          severity: error
      - receiver: 'default-receiver'
        match:
          severity: warning
      - receiver: 'default-receiver'
        match:
          severity: info
      - receiver: 'null'
        matchers:
          - alertname = "Watchdog"
    receivers:
    - name: 'null'
    - name: 'default-receiver'
      slack_configs:
      - api_url: 'https://slack.com/api/chat.postMessage'
        channel: '#${alertmanager_slack_channel}'
        send_resolved: true
        http_config:
          authorization:
            credentials: ${alertmanager_slack_credentials}
        color: '{{ if eq .Status "firing" }}warning{{ else }}good{{ end }}'
        title: '[{{ if eq .Status "firing" }}{{ .Alerts.Firing | len }} {{ end }} {{ if
          eq .Status "firing" }}Alerts{{ else }}{{ .Status | toUpper }}{{ end }}]'
        title_link: '{{ template "slack.default.titlelink" . }}'
        pretext: '{{ .CommonAnnotations.summary }}'
        text: |-
          {{ range .Alerts }}
          {{- if .Annotations.summary }}*Alert:* {{ .Annotations.summary }}{{- end }}
          *Description:* {{ .Annotations.description }}{{ .Annotations.message }}
          *Severity:* `{{ .Labels.severity }}`
          *Source:* Prometheus Alertmanager
          {{ end }}
        short_fields: false
        footer: '{{ template "slack.default.footer" . }}'
        fallback: '{{ template "slack.default.fallback" . }}'
        callback_id: '{{ template "slack.default.callbackid" . }}'
        icon_emoji: '{{ template "slack.default.iconemoji" . }}'
        icon_url: '{{ template "slack.default.iconurl" . }}'
        link_names: false
    templates:
    - '/etc/alertmanager/config/*.tmpl'
  ingress:
    enabled: true
    ingressClassName: ${ingress_class_name}
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt-prod
      nginx.ingress.kubernetes.io/auth-url: "https://${auth_oauth2_proxy_host}/oauth2/auth"
      nginx.ingress.kubernetes.io/auth-signin: "https://${auth_oauth2_proxy_host}/oauth2/start?rd=$scheme://$host$escaped_request_uri"
      nginx.ingress.kubernetes.io/auth-response-headers: "X-Auth-Request-User,X-Auth-Request-Email,X-Auth-Request-Access-Token"
    labels: {}
    hosts:
      - ${alertmanager_domain}
    paths:
      - /
    pathType: ImplementationSpecific
    %{ if ingress_enable_tls }
    tls:
      - secretName: alertmanager-general-tls
        hosts:
        - ${alertmanager_domain}
    %{ endif }

# https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml
grafana:
  enabled: true
  defaultDashboardsTimezone: Australia/Melbourne
  adminPassword: ${grafana_admin_password}
  datasources: {}
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - name: 'default'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards/default
  dashboards:
    default:
      minio-server:
        url: https://raw.githubusercontent.com/minio/minio/master/docs/metrics/prometheus/grafana/minio-dashboard.json
        datasource:
        - name: DS_PROMETHEUS
          value: Prometheus
      minio-bucket:
        url: https://raw.githubusercontent.com/minio/minio/master/docs/metrics/prometheus/grafana/bucket/minio-bucket.json
        datasource:
        - name: DS_PROMETHEUS
          value: Prometheus
      minio-node:
        url: https://raw.githubusercontent.com/minio/minio/master/docs/metrics/prometheus/grafana/node/minio-node.json
        datasource:
        - name: DS_PROMETHEUS
          value: Prometheus
      longhorn: # https://grafana.com/grafana/dashboards/16888-longhorn/
        gnetId: 16888
        revision: 9
        datasource: Prometheus
      ingress-nginx:
        url: https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/grafana/dashboards/nginx.json
      ingress-nginx-request-handling-performance:
        url: https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/grafana/dashboards/request-handling-performance.json

  ingress:
    enabled: true
    ingressClassName: ${ingress_class_name}
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt-prod
      nginx.ingress.kubernetes.io/auth-url: "https://${auth_oauth2_proxy_host}/oauth2/auth"
      nginx.ingress.kubernetes.io/auth-signin: "https://${auth_oauth2_proxy_host}/oauth2/start?rd=$scheme://$host$escaped_request_uri"
      nginx.ingress.kubernetes.io/auth-response-headers: "X-Auth-Request-User,X-Auth-Request-Email,X-Auth-Request-Access-Token"
    labels: {}
    hosts:
      - ${grafana_domain}
    path: /
    %{ if ingress_enable_tls }
    tls:
    - secretName: grafana-general-tls
      hosts:
      - ${grafana_domain}
    %{ endif }

  # To make Grafana persistent (Using Statefulset)
  #
  persistence:
    enabled: true
    type: sts
    storageClassName: ${persistence_storage_class_name}
    accessModes:
      - ReadWriteOnce
    size: ${persistence_size}
    finalizers:
      - kubernetes.io/pvc-protection

# https://github.com/prometheus-community/helm-charts/blob/main/charts/prometheus/values.yaml
prometheus:
  enabled: true
  prometheusSpec:
    ## ServiceMonitors to be selected for target discovery.
    ## If matchLabels.release: "{{ $.Release.Name }}" the prometheus resource will be created
    ## with selectors based on values in the helm deployment, which will also match the scrapeConfigs created
    ## To remove matchLabels from the selector condition, explicitly set matchLabels to null.
    ## If no other selectors are configured, prometheus-operator will select all scrapeConfigs.
    ## To remove the release label from the matchLabels condition, explicit set release to null.
    ## If null, exclude the field from the prometheusSpec (keeping downward compatibility with older versions of CRD)
    ##
    serviceMonitorSelector:
      matchLabels: null

    ## PodMonitors to be selected for target discovery.
    ## If matchLabels.release: "{{ $.Release.Name }}" the prometheus resource will be created
    ## with selectors based on values in the helm deployment, which will also match the scrapeConfigs created
    ## To remove matchLabels from the selector condition, explicitly set matchLabels to null.
    ## If no other selectors are configured, prometheus-operator will select all scrapeConfigs.
    ## To remove the release label from the matchLabels condition, explicit set release to null.
    ## If null, exclude the field from the prometheusSpec (keeping downward compatibility with older versions of CRD)
    ##
    podMonitorSelector:
      matchLabels: null
    ## Example which selects PodMonitors with label "prometheus" set to "somelabel"
    # podMonitorSelector:
    #   matchLabels:
    #     prometheus: somelabel

    ## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations
    ## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form
    ## as specified in the official Prometheus documentation:
    ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config. As scrape configs are
    ## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility
    ## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible
    ## scrape configs are going to break Prometheus after the upgrade.
    ## AdditionalScrapeConfigs can be defined as a list or as a templated string.
    ##
    ## The scrape configuration example below will find master nodes, provided they have the name .*mst.*, relabel the
    ## port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes
    additionalScrapeConfigs:
    # Note:
    #   curl http://minio.minio-tenant.svc.cluster.local/minio/v2/metrics/cluster
    - job_name: minio-job
      bearer_token: ${minio_job_bearer_token}
      metrics_path: /minio/v2/metrics/cluster
      scheme: http
      static_configs:
      - targets: [minio.minio-tenant.svc.cluster.local]
    - job_name: minio-job-node
      bearer_token: ${minio_job_node_bearer_token}
      metrics_path: /minio/v2/metrics/node
      scheme: http
      static_configs:
      - targets: [minio.minio-tenant.svc.cluster.local]
    - job_name: minio-job-bucket
      bearer_token: ${minio_job_bucket_bearer_token}
      metrics_path: /minio/v2/metrics/bucket
      scheme: http
      static_configs:
      - targets: [minio.minio-tenant.svc.cluster.local]
    - job_name: minio-job-resource
      bearer_token: ${minio_job_resource_bearer_token}
      metrics_path: /minio/v2/metrics/resource
      scheme: http
      static_configs:
      - targets: [minio.minio-tenant.svc.cluster.local]
  ingress:
    enabled: true
    ingressClassName: ${ingress_class_name}
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt-prod
      nginx.ingress.kubernetes.io/auth-url: "https://${auth_oauth2_proxy_host}/oauth2/auth"
      nginx.ingress.kubernetes.io/auth-signin: "https://${auth_oauth2_proxy_host}/oauth2/start?rd=$scheme://$host$escaped_request_uri"
      nginx.ingress.kubernetes.io/auth-response-headers: "X-Auth-Request-User,X-Auth-Request-Email,X-Auth-Request-Access-Token"
    labels: {}
    hosts:
      - ${prometheus_domain}
    paths:
    - /
    pathType: ImplementationSpecific
    %{ if ingress_enable_tls }
    tls:
      - secretName: prometheus-general-tls
        hosts:
          - ${prometheus_domain}
    %{ endif }
